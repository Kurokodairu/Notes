
1:
STOCHASTIC GRADIENT DECENT (GSD)

FORWARD PROPAGATION
BACK PROPAGATION

SIGMOID FUNCTION 
OTHERS?


2:


3 CNN:
- Convolution, Relu
- Image generator

- Residual Networks
- Batch Normalization
- Inception networks, 1x1 convolution trick
- Max pooling / Average pooling
- Fully Convolutional Network 

	Covolution, max pooling (relu) -> FEATURE EXTRACTION
	Fully connected dense layers -> Classification

4: Generative models
**Encoder**: This part of the network compresses the input into a latent-space representation. It encodes the input data as an internal fixed-size representation in reduced dimensionality.  
• **Latent Space:** This is the compressed representation of the input data. It holds the key features  necessary to reconstruct the input data.  
• **Decoder**: This part of the network reconstructs the input data from the internal representation. It maps the encoded data back to the original space

- mean vector, standard deviation vector => Z
- ![[Pasted image 20251111222356.png]]


5: Sentiment Analysis
