{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce532dc",
   "metadata": {},
   "source": [
    "\n",
    "#  Assignment: Binary Classification with Forward and Backward Propagation (MSE Loss)  based on the lecture **01 - ANN from scratch**(ANN.ipynb - Backward Propagation section).\n",
    "\n",
    "## Objectives\n",
    "- Implement **forward propagation** for a binary neural network.\n",
    "- Implement **backward propagation** using the **full chain rule** (with sigmoid derivatives kept explicitly).\n",
    "- Train on MNIST (digits 0 vs 1) using only **NumPy**.\n",
    "- Use **sigmoid activations** and **Mean Squared Error (MSE) loss**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Definitions\n",
    "\n",
    "- **Sigmoid function (Ïƒ):**\n",
    "  $$\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  $$\n",
    "\n",
    "- **Mean Squared Error (MSE):**\n",
    "  $$\n",
    "  L = \\frac{1}{2m} \\sum_{i=1}^m \\Big( y^{(i)} - a^{(out)(i)} \\Big)^2\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7647d",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Feedforward Equations\n",
    "\n",
    "- Hidden layer:\n",
    "  $$\n",
    "  z^{(h)} = X W^{(h)T} + b^{(h)}, \\quad a^{(h)} = \\sigma(z^{(h)})\n",
    "  $$\n",
    "\n",
    "- Output layer:\n",
    "  $$\n",
    "  z^{(out)} = a^{(h)} W^{(out)T} + b^{(out)}, \\quad a^{(out)} = \\sigma(z^{(out)})\n",
    "  $$\n",
    "\n",
    " **Task:** Implement `forward()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward(X, W_h, b_h, W_out, b_out):\n",
    "    z_h = np.dot(X, W_h) + b_h\n",
    "    a_h = sigmoid(z_h)\n",
    "    z_out = np.dot(a_h, W_out) + b_out\n",
    "    a_out = sigmoid(z_out)\n",
    "    return z_h, a_h, z_out, a_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c9395",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Backward Equations (with MSE + sigmoid)\n",
    "\n",
    "We compute gradients using the chain rule, **without cancelling the sigmoid derivative**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Loss derivative\n",
    "- Loss derivative w.r.t. output activation:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a^{(out)}} = (a^{(out)} - y)\n",
    "$$\n",
    "\n",
    "### Step 2: Sigmoid derivative\n",
    "$$\n",
    "\\frac{\\partial a^{(out)}}{\\partial z^{(out)}} = a^{(out)} (1 - a^{(out)})\n",
    "$$\n",
    "\n",
    "### Step 3: Output error term\n",
    "$$\n",
    "\\delta^{(out)} = (a^{(out)} - y) \\cdot a^{(out)} (1 - a^{(out)})\n",
    "$$\n",
    "\n",
    "### Step 4: Gradients for output layer\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(out)}} = (a^{(h)})^T \\delta^{(out)},\n",
    "\\quad\n",
    "\\frac{\\partial L}{\\partial b^{(out)}} = \\sum \\delta^{(out)}\n",
    "$$\n",
    "\n",
    "### Step 5: Hidden error term\n",
    "$$\n",
    "\\delta^{(h)} = (\\delta^{(out)} W^{(out)}) \\odot a^{(h)} (1 - a^{(h)})\n",
    "$$\n",
    "\n",
    "### Step 6: Gradients for hidden layer\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(h)}} = X^T \\delta^{(h)},\n",
    "\\quad\n",
    "\\frac{\\partial L}{\\partial b^{(h)}} = \\sum \\delta^{(h)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    " **Task:** Implement `backward()` accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e94c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def backward(X, y, z_h, a_h, z_out, a_out, W_out):\n",
    "    # Output error with sigmoid derivative explicit\n",
    "    delta_out = (a_out - y) * sigmoid_derivative(a_out)\n",
    "    dW_out = np.dot(a_h.T, delta_out)\n",
    "    db_out = np.sum(delta_out, axis=0, keepdims=True)\n",
    "    \n",
    "    # Hidden error\n",
    "    delta_h = np.dot(delta_out, W_out.T) * sigmoid_derivative(a_h)\n",
    "    dW_h = np.dot(X.T, delta_h)\n",
    "    db_h = np.sum(delta_h, axis=0, keepdims=True)\n",
    "    \n",
    "    return dW_h, db_h, dW_out, db_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaca7f6",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Training Function\n",
    "\n",
    "We will train using stochastic gradient descent (SGD).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53bac1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(X, y, hidden_dim=64, epochs=50, lr=0.01):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_outputs = 1  # binary output\n",
    "\n",
    "    # Initialize parameters\n",
    "    W_h = np.random.randn(n_features, hidden_dim) * 0.01\n",
    "    b_h = np.zeros((1, hidden_dim))\n",
    "    W_out = np.random.randn(hidden_dim, n_outputs) * 0.01\n",
    "    b_out = np.zeros((1, n_outputs))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward\n",
    "        z_h, a_h, z_out, a_out = forward(X, W_h, b_h, W_out, b_out)\n",
    "\n",
    "        # Backward\n",
    "        dW_h, db_h, dW_out, db_out = backward(X, y, z_h, a_h, z_out, a_out, W_out)\n",
    "\n",
    "        # Parameter updates\n",
    "        W_h -= lr * dW_h\n",
    "        b_h -= lr * db_h\n",
    "        W_out -= lr * dW_out\n",
    "        b_out -= lr * db_out\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            loss = np.mean(0.5 * (y - a_out)**2)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return W_h, b_h, W_out, b_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080ab7b",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Test on MNIST (Digits 0 and 1 Only)\n",
    "\n",
    " Use the block below to:\n",
    "1. Load MNIST using `sklearn.datasets.fetch_openml`\n",
    "2. Keep only digits 0 and 1\n",
    "3. Normalize inputs to [0,1]\n",
    "4. Train the model on a subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad67dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.1251\n",
      "Epoch 10, Loss: 0.2427\n",
      "Epoch 20, Loss: 0.2427\n",
      "Test Accuracy: 0.5257104194857916\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
    "\n",
    "# Select only digits 0 and 1\n",
    "mask = (y == 0) | (y == 1)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "# Normalize\n",
    "X = X / 255.0\n",
    "y = y.to_numpy().reshape(-1, 1)  \n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model on subset\n",
    "W_h, b_h, W_out, b_out = train(X_train[:2000], y_train[:2000], hidden_dim=32, epochs=30, lr=0.5)\n",
    "\n",
    "# Evaluate\n",
    "_, _, _, a_out_test = forward(X_test, W_h, b_h, W_out, b_out)\n",
    "y_pred = (a_out_test >= 0.5).astype(int)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce245ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
