FOLDER
DAT300/01/01-ANN FROM SCRATCH/ANN.ipynb

## Single-layer neural network recap### ADAptive LInear NEuron (Adaline)
Defined for a two-class problem.

![[Pasted image 20250908125035.png]]

### Net input:
$z = \sum_j w_j x_j = \bf{w^T x}$

### Activation function
For Adaline:
The identity activation function simplifies the learning algorithm and allows for straightforward weight updates based on the linear relationship between the inputs and the target outputs

$\phi (z) = z = a$

For Logistic Regression:
$\phi (z) = \frac{1}{1+e^{-z}} = a$

$\phi$ : Linear (Adalane)
$\phi$ : Sigmoid: Logistic regression

### Threshold for Adaline:
### Threshold (for Adaline):  
$$
\begin{equation}
  \hat{y}=\begin{cases}
    1  \text{ if } g(z) \geq 0\\
    -1  \text{ otherwise}.
  \end{cases}
\end{equation}
$$

### Weight update
Gradient descent:
$\bf{w:=w} + \Delta \bf{w}$, where $\Delta \bf{w} = -\eta \nabla \it{J} \bf{(w)}$,
i.e. the gradient based on the whole training set, taking a step opposite to the gradient.
- Remebmer: The gradient of a function indicates the direction in which the function's value increases the fastest

Cost function, sum of squared errors (SSE):
$\it{J} \bf{(w)} = \frac{1}{2}\sum^n_{i=1} (y^{(i)}-\phi (z^{(i)}))^2$

Partial derivative:
$\frac{\partial}{\partial w_j} \it{J} (\bf{w}) = -\sum_i (y^{(i)} - \phi (z^{(i)}))x^{(i)}_j$,
where (y, x, a) are (target label, sample and activation).




We can save the weights:
ada.W_
[array([0.73258132, 0.97496049]), array([-0.06382416, 0.37479879]), array([0.42378193, 1.02184467]), array([-0.10191557, 0.62568527]), array([0.21994724, 1.05279234]), array([-0.12705925, 0.79129237]), array([0.08539845, 1.07322051]), array([-0.14365628, 0.90060759]), array([-0.00341556, 1.0867049 ]), array([-0.15461178, 0.97276523]), array([-0.06204059, 1.09560579]), array([-0.16184337, 1.0203956 ]), array([-0.10073826, 1.10148115]), array([-0.16661685, 1.05183582]), array([-0.12628212, 1.1053594 ])]



### Optimisation
- Gradient descent
- Stochastic gradient descent (SGD)
- Online learning (random, single sample update)
- Mini-batch learning (random subset update)




# Multi layer neural network

#### Multi layer perceptron (MLP)
![[Pasted image 20250908132727.png]]
- $x_i^{(in)}$ = the $i$th  input feature value
- $a_i^{(in)}$ =  the $i$th  unit in the input layer. 
- $a_i^{(h)}$ =  the $i$th  unit in the hidden layer. 
- $a_i^{(out)}$ =  the $i$th  unit in the output layer. 
- $a_i^{(l)}$ = the activation unit $i$ in $l$-th layer. 
- $w_{i,j}^{(l)}$ = weight connecting unit $i$ in $l-1$-th layer with unit $j$ in $l$-th layer. These will be grouped into a matrix $W^{(l)}$ later. 
- $i=0$ are bias units (constant/intercept).  
More than one hidden layer => **deep artificial neural network**.
### First layer activation

$a^{(in)} = \begin{bmatrix}a^{(in)}_0 \\ a^{(in)}_1 \\ \vdots \\ a^{(in)}_m\end{bmatrix} = \begin{bmatrix}1 \\ x^{(in)}_1 \\ \vdots \\ x^{(in)}_m\end{bmatrix}$,

i.e. the input and an intercept.
The bias units of all layers are grouped together as a separate vector later.

### Adding layers
- More layers give higher flexibility
    - Weight matrices, $W^{(l)}$, will have dimension $m \times d$, where $m$ is the number of units in the previous layer (including the bias) and $d$ is the number of units in the $l$-th layer.
- Increases possibility of over-fitting
- Increases number of parameters to estimate (good and bad)
- Error gradients increasingly small
    - Deep learning tools/tricks to overcome this later

### Multiclass classification
- Multiple output units
- Corresponds to one-hot encoding
- Typically a soft-max function on the output:
  - Class probabilities

## Activating a neural network via forward propagation
  Three main steps of fitting an MLP:
1. Forward propagate patterns of the training data from input to output.
2. Calculate error based on a cost function.
3. Backpropagate the error through derivatives with respect to each weight and update the model.  

Repeat for several epochs.
### 1. Forward propagation
Calculation of the first actiavation unit in the (first) hidden layer:  
$$z_1^{(h)} = a_0^{(in)} w_{0,1}^{(h)} + a_1^{(in)} w_{1,1}^{(h)} + \dots + a_m^{(in)} w_{m,1}^{(h)}$$  
$$a_1^{(h)} = \phi(z_1^{(h)})$$
$\phi(\cdot)$ is the activation function, differentiable, often non-linear.

### Activation function examples
The basic sigmoid transforms unbounded inputs $z$ to the range $(0,1)$:
![[Pasted image 20250908132912.png]]
Rectified Linear Units - ReLU (and its siblings) have taken over as the default activiation function in many applications due to their simplicity and beneficial properties.

## Compact notation
For easier readability and more efficient calculations we change from sum-notation to matrix notation. Collect all activation units of layer $l-1$ and all weights for the transition to layer $l$ into matrices:  
$$\bf{Z}^\it{(l)} = \bf{A}^\it{(l-1)} \bf{W}^\it{(l)}$$
$$\bf{A}^\it{(l)} = \phi \left(\bf{Z}^\it{(l)} \right)$$





# Training an artificial neural network
- The cost function
- Backpropagation
- Convergence

## Logistic Cost Function (Cross-Entropy)

- For binary labels $y \in \{0,1\}$, we model probability with sigmoid:  
  $$ a^{(i)} = \sigma(z^{(i)}) = \frac{1}{1+e^{-z^{(i)}}} $$
- Bernoulli likelihood (one sample):  
  $$ P(y^{(i)}|x^{(i)},w) = (a^{(i)})^{y^{(i)}} (1-a^{(i)})^{(1-y^{(i)})} $$
- Log-likelihood (dataset):  
  $$ \ell(w) = \sum_{i=1}^n \Big[ y^{(i)}\log a^{(i)} + (1-y^{(i)})\log(1-a^{(i)}) \Big] $$
- Cost = Negative log-likelihood:  
  $$ J(w) = -\ell(w) = -\sum_{i=1}^n \Big[ y^{(i)}\log a^{(i)} + (1-y^{(i)})\log(1-a^{(i)}) \Big] $$

## Computing the logistic cost function
The implementation used the same logistic cost functions as Chapter 3 as a basis:
$$\it{J} (w) = -\sum_{i=1}^{n} y^{[i]} log(a^{[i]}) + \left( 1-y^{[i]}\right) log(1-a^{[i]})$$
... where $a^{[i]}$ is output of a node for sample [i] in the output layer:
$$a^{[i]} = \phi(z^{[i]})$$
### L2 penalization
$$L2 = \lambda ||w||^2_2 = \lambda \sum_{j=1}^m w^2_j,$$
regularizes all weights except the bias units for the node. 

Adding this to the cost results in:
  
$$\it{J} (w) = -\left[ \sum_{i=1}^{n} y^{[i]} log(a^{[i]}) + \left( 1-y^{[i]}\right) log(1-a^{[i]}) \right] + \frac{\lambda}{2} ||w||^2_2$$
### Cost function for all samples and nodes
$$\it{J} (w) = -\sum_{i=1}^{n}\sum_{j=1}^{t} y_j^{[i]} log(a_j^{[i]}) + \left( 1-y_j^{[i]}\right) log(1-a_j^{[i]})$$
Adding the L2 norm penalization finally yields:
$$\it{J} (w) = -\left[ \sum_{i=1}^{n}\sum_{j=1}^{t} y_j^{[i]} log(a_j^{[i]}) + \left( 1-y_j^{[i]}\right) log(1-a_j^{[i]}) \right] + \frac{\lambda}{2} \sum_{l=1}^{L-1}\sum_{i=1}^{u_l}\sum_{j=1}^{u_{l+1}}\left(w_{j,i}^{(l)}\right)^2$$
($u_l$ is the number of units in a layer)

```python
def _compute_cost(self, y_enc, output): # where y_enc = y and output = a
    L2_term = (self.l2 *
               (np.sum(self.w_h ** 2.) +
                np.sum(self.w_out ** 2.)))

    term1 = -y_enc * (np.log(output))
    term2 = (1. - y_enc) * np.log(1. - output)
    cost = np.sum(term1 - term2) + L2_term
    return cost
```

### One-hot encoding representation
The activation of the third layer and the target class for __a particular__ sample may look like this:

$$a^{(out)} = \begin{bmatrix}0.1 \\ 0.9 \\ \vdots \\ 0.3\end{bmatrix}, y = \begin{bmatrix}0 \\ 1 \\ \vdots \\ 0\end{bmatrix}$$
```python
def predict(self, X):
	z_h, a_h, z_out, a_out = self._forward(X)
	y_pred = np.argmax(z_out, axis=1) # argmax on z_out and a_out is equivalent
	return y_pred
```
### Cost minimization
... will be performed using the partial derivative of the parameters of $W$ with respect to each weight for every layer:
$$\frac{\partial}{\partial w_{j,i}^{(l)}}J(W)$$
## Developing your intuition for backpropagation
- Backpropagation is still one of the most popular algorithms to train ANNs
- Challenging with high dimensions and non-convex cost function (local minima)
- Main building block: "The chain rule"
    - Derivative of a nested function
$$\frac{d}{dx}[f(g(x))] = \frac{df}{dg}\cdot\frac{dg}{dx}$$
$$F(x) = f(g(h(u(v(x)))))$$
$$\frac{dF}{dx} = \frac{d}{dx}F(x) = \frac{d}{dx}[f(g(h(u(v(x)))))] = \frac{df}{dg}\cdot\frac{dg}{dh}\cdot\frac{dh}{du}\cdot\frac{du}{dv}\cdot\frac{dv}{dx}$$
The derivative of the sigmoid function:  
$$\phi'(z) = \frac{\partial}{\partial z} \left( \frac{1}{1+e^{-z}} \right) = \frac{1}{1+e^{-z}} - \left( \frac{1}{1+e^{-z}} \right)^2$$
$$= \phi(z) - (\phi(z))^2 = \phi(z)(1-\phi(z)) = a(1-a)$$
Proof: https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e

## Training neural networks via backpropagation
Let's assume that the loss is the Mean Squared Error, which involves averaging the activation units within our network, in addition to averaging over the n training examples
$$\it{L} (w,b) = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{t}\sum_{j=1}^{t} (y_j^{[i]} - a_j^{(out)[i]})^{2}$$
## Training neural networks via backpropagation
- Gradients represent how much a small change in a parameter (such as a weight) affects the output of a function (such as the loss function). how much a small change in a parameter (such as a weight) 
- When you calculate the gradient during backpropagation, it involves multiplying the error (a measure of how far the network's prediction is from the actual target) by the weights. This multiplication represents how much the weights contributed to the error in the output.
### Weight update
$$W^{(l)}:=W^{(l)} - \eta \Delta^{(l)}$$
A small step in the opposite direction of the gradient (for each layer in the network).

# Convergence in neural networks
- Gradient descent with mini-batch instead of stochastic gradient descent (Ch. 2, p.47)
    - Efficient vectorized code
    - Like a "meningsmåling ved valg" / election poll
    - Still stochastic, hopefully jumping out of local minima
- Learning rate: fixed rate $\eta$
    - Deep Learning brings adaptive learning rate, learning rate with decay, learning rate with restarts, ...
![[Pasted image 20250911180904.png]]
