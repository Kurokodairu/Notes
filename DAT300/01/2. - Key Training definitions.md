
---
## Gradient Descent (GD)
- Optimization algorithm to minimize a loss function.
- Uses the **entire dataset** to compute the gradient before each parameter update.
- Accurate but **slow** for large datasets.
---
## Stochastic Gradient Descent (SGD)
- Updates parameters using **only one sample at a time**.
- Much faster, but noisy (gradient fluctuates).
- Often helps avoid local minima.
---
## Mini-Batch Gradient Descent
- **Compromise between batch and SGD.**
- Uses a **small subset** of the data (e.g., 32, 64, 128 samples) per update.
- Most common method in deep learning.
- Benefits:
  - Faster than batch GD
  - Smoother convergence than pure SGD
  - Works well with GPUs
---
## Batch
- A **subset of the training data** used in one forward + backward pass.
- Types:
- **Full batch** → all data at once (classic GD).
- **Mini-batch** → small group of samples (e.g., 32 or 64).
- **Batch size = 1** → pure SGD.
---
## Iteration
- **One parameter update step**.
- If you have 1000 samples and batch size = 100 →  
  1 iteration = 100 samples,  
  10 iterations = full dataset (1 epoch).
---
## Epoch
- **One full pass** through the training dataset.
- If batch size = 100 and dataset = 1000 samples →  
  1 epoch = 10 iterations.
---
#  Example: How Many Weight Updates?
Suppose we have:
- Training samples = **10,000**
- Batch size = **100**
- Epochs = **5**
---
### Step 1: Iterations per epoch
- Each iteration processes **1 batch**.
- Number of batches = 10,000 ÷ 100 = **100 iterations**.
---
### Step 2: Total iterations (weight updates)
- 1 epoch = 100 updates
- 5 epochs = 5 × 100 = **500 weight updates**
---