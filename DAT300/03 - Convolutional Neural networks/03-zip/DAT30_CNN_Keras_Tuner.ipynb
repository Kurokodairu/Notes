{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-qCmyrN_a_3"
   },
   "source": [
    "**Keras Tuner Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z1jFRRBb9_Q0",
    "outputId": "0b49c55d-14e7-4f8b-ea79-42abd426dbc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.9.2)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.11.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 913
    },
    "id": "2k9XcELe97CL",
    "outputId": "d64ce69f-12dc-41d5-c62b-45ed24218cce"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.7' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/kuro/Documents/Notes/DAT300/.venv/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_tuner import RandomSearch  # Import Keras Tuner's RandomSearch\n",
    "\n",
    "# 1. Load the data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 2. Preprocess the data\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# 3. Split the data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Define the model-building function for Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Tune the number of layers and units per layer\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "            activation=hp.Choice('activation', ['relu', 'tanh'])\n",
    "        ))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# 5. Initialize the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # Number of models to build and fit for each trial\n",
    "    directory='mnist_tuner_dir',\n",
    "    project_name='mnist_tuner'\n",
    ")\n",
    "\n",
    "# 6. Perform the hyperparameter search\n",
    "tuner.search(\n",
    "    train_images, train_labels,\n",
    "    epochs=10,\n",
    "    validation_data=(val_images, val_labels)\n",
    ")\n",
    "\n",
    "# 7. Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# 8. Evaluate the best model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# 9. Plot training & validation accuracy and loss values\n",
    "# Since Keras Tuner does not store the history object, retrain the best model to get the history\n",
    "history = best_model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=10,\n",
    "    validation_data=(val_images, val_labels)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linestyle='--')\n",
    "plt.title('Best Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linestyle='--')\n",
    "plt.title('Best Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtYVNClx_VWK"
   },
   "source": [
    "**Early Stoping Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUZxsyHP_Tj7",
    "outputId": "4ad8057e-d602-4499-f6e4-70a266e5e807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.8807 - loss: 0.4104 - val_accuracy: 0.9645 - val_loss: 0.1204\n",
      "Epoch 2/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.9678 - loss: 0.1095 - val_accuracy: 0.9698 - val_loss: 0.1084\n",
      "Epoch 3/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9801 - loss: 0.0678 - val_accuracy: 0.9770 - val_loss: 0.0796\n",
      "Epoch 4/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9854 - loss: 0.0475 - val_accuracy: 0.9789 - val_loss: 0.0743\n",
      "Epoch 5/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9901 - loss: 0.0321 - val_accuracy: 0.9787 - val_loss: 0.0794\n",
      "Epoch 6/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9933 - loss: 0.0241 - val_accuracy: 0.9760 - val_loss: 0.0901\n",
      "Epoch 7/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9946 - loss: 0.0191 - val_accuracy: 0.9790 - val_loss: 0.0751\n",
      "Epoch 8/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9960 - loss: 0.0138 - val_accuracy: 0.9801 - val_loss: 0.0815\n",
      "Epoch 9/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9977 - loss: 0.0093 - val_accuracy: 0.9829 - val_loss: 0.0710\n",
      "Epoch 10/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9982 - loss: 0.0071 - val_accuracy: 0.9814 - val_loss: 0.0787\n",
      "Epoch 11/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9985 - loss: 0.0057 - val_accuracy: 0.9798 - val_loss: 0.0917\n",
      "Epoch 12/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.9995 - loss: 0.0030 - val_accuracy: 0.9823 - val_loss: 0.0837\n",
      "Epoch 13/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.9781 - val_loss: 0.1013\n",
      "Epoch 14/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.9827 - val_loss: 0.0824\n",
      "Epoch 15/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 6.6465e-04 - val_accuracy: 0.9820 - val_loss: 0.0861\n",
      "Epoch 16/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.0366e-04 - val_accuracy: 0.9836 - val_loss: 0.0841\n",
      "Epoch 17/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 2.8370e-04 - val_accuracy: 0.9830 - val_loss: 0.0858\n",
      "Epoch 18/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1712e-04 - val_accuracy: 0.9828 - val_loss: 0.0862\n",
      "Epoch 19/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4461e-04 - val_accuracy: 0.9825 - val_loss: 0.0870\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.0822\n",
      "\n",
      "Test accuracy: 0.9815000295639038\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load the data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Create validation data\n",
    "val_images = train_images[50000:]\n",
    "val_labels = train_labels[50000:]\n",
    "train_images = train_images[:50000]\n",
    "train_labels = train_labels[:50000]\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10, # Number of epochs with no improvement after which training will be stopped.\n",
    "    verbose=1,\n",
    "    restore_best_weights=True # Restores model weights from the epoch with the best value of the monitored quantity.\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "# Test the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
