{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8b0d1366-6552-473d-9e29-2a9b1f283b8e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallelising Neural Network Training with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing activation functions for multilayer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Technically, one can use **any function** as an activation function in multilayer neural networks as long as it is **differentiable**\n",
    "* One could can even use **linear** activation functions, such as in Adaline, but ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- would **not** be very useful to use linear activation functions for both hidden and output layers \n",
    "- to tackle complex problems one needs to introduce **non-linearity**\n",
    "- the **sum of linear functions** would yield only **another** linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Activation Functions: Pros and Cons\n",
    "\n",
    "| **Activation Function** | **Pros**                               | **Cons**                                    |\n",
    "|-------------------------|----------------------------------------|---------------------------------------------|\n",
    "| **Linear**              | Simple, good for regression            | No non-linearity, cannot handle complex data|\n",
    "| **Unit Step**           | Simple, useful in binary classification| Not differentiable, poor for gradient-based learning |\n",
    "| **Sign**                | Easy to compute                        | Non-differentiable, limits model complexity |\n",
    "| **Piece-wise Linear**    | Used in specific algorithms (SVMs)     | More complex, limited use in neural networks|\n",
    "| **Logistic (Sigmoid)**  | Smooth output, good for probabilities  | Vanishing gradient problem, slow convergence|\n",
    "| **Tanh (Hyperbolic Tangent)** | Zero-centered output, stronger gradient | Suffers from vanishing gradients          |\n",
    "| **ReLU**                | Efficient, fast convergence, avoids vanishing gradients | Can \"die\" for negative inputs, not zero-centered |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"./images/overview_actfunc.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The **logistic activation function** (which we often called *sigmoid function*) mimics the concept of a neuron in a brain most closely - think of it as the probability of whether a neuron fires or not\n",
    "* However, logistic activation functions can be problematic\n",
    "    - When net input $\\textbf{z}$ is **highly negative**, $\\phi{(\\textbf{z})}$ would be close to zero\n",
    "    - If $\\phi{(\\textbf{z})}$ is close to zero the neural network would learn **very slowly**\n",
    "    - More slowly learning could lead to the neural network **getting trapped in local minima** during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating class probabilities in multiclass classification via the ``softmax`` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In previous sections: obtain a class label using the ``argmax`` function\n",
    "* The ``softmax`` function is in fact a soft form of the ``argmax`` function; instead of giving a single class index, it provides the **probability of each class**\n",
    "* The ``softmax`` function allows for computation of **meaningful** class probabilities in **multiclass** settings (multinomial logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In ``softmax``, the probability of a particular sample with net input $z$ belonging to the $i$th class can be computed with a normalization term in the denominator, that is, the sum of all $M$ linear functions.\n",
    "We do not use Softmax in hidden layers: If a hidden layer has multiple neurons, each neuron is supposed to activate independently, meaning they respond to different patterns in the input. However, if we use softmax in a hidden layer, it forces the neurons to compete with each other because their outputs must add up to 1. This would interfere with their job of learning different features from the data and limit the flexibility of the network to learn effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    " <img src=\"./images/softmax_eq.png\" width=\"500\"/>\n",
    " \n",
    " - $p(y = i \\mid z)$  \n",
    "  → \"The probability that the output $y$ equals class $i$, given the input vector $z$.\"\n",
    "\n",
    "- $e^{z_i}$  \n",
    "  → \"Take the exponential of the score (logit) $z_i$ for class $i$.\"\n",
    "\n",
    "- $\\sum_{j=1}^{M} e^{z_j}$  \n",
    "  → \"Compute the sum of exponentials of all class scores $z_j$, where $M$ is the total number of classes.\"\n",
    "\n",
    "- $\\frac{e^{z_i}}{\\sum_{j=1}^{M} e^{z_j}}$  \n",
    "  → \"Divide the exponential of class $i$ by the sum of exponentials of all classes, to normalize into a probability.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``softmax`` function coded in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n",
      " [0.65900114 0.24243297 0.09856589]\n",
      "Sum of probabilities: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    # subtract max for numerical stability\n",
    "    exp_z = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "\n",
    "# Example input vector (logits)\n",
    "Z = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "y_probas = softmax(Z)\n",
    "print(\"Probabilities:\\n\", y_probas)\n",
    "print(\"Sum of probabilities:\", np.sum(y_probas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Example: Softmax with 3 values\n",
    "\n",
    "We want to compute the softmax for:\n",
    "\n",
    "$z = [2.0, \\; 1.0, \\; 0.1]$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Exponentiate each value\n",
    "\n",
    "$e^{2.0} \\approx 7.389$  \n",
    "$e^{1.0} \\approx 2.718$  \n",
    "$e^{0.1} \\approx 1.105$\n",
    "\n",
    "So:  \n",
    "\n",
    "$e^z = [7.389, \\; 2.718, \\; 1.105]$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Compute denominator (sum of exponentials)\n",
    "\n",
    "$\\text{denominator} = 7.389 + 2.718 + 1.105 = 11.212$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Step 3: Divide each exponential by denominator\n",
    "\n",
    "$\\text{softmax}(2.0) = \\frac{7.389}{11.212} \\approx 0.659$  \n",
    "\n",
    "$\\text{softmax}(1.0) = \\frac{2.718}{11.212} \\approx 0.242$  \n",
    "\n",
    "$\\text{softmax}(0.1) = \\frac{1.105}{11.212} \\approx 0.099$\n",
    "\n",
    "---\n",
    "\n",
    "###  Final Result\n",
    "\n",
    "$\\text{softmax}(z) = [0.659, \\; 0.242, \\; 0.099]$\n",
    "\n",
    "Notice:  \n",
    "\n",
    "$0.659 + 0.242 + 0.099 \\approx 1.0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Broadening the output spectrum using a hyperbolic tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Another *sigmoid function* that is often used in the **hidden layers** of artificial neural networks is the **hyperbolic tangent** (commonly known as ``tanh``)\n",
    "* ``tanh`` can be interpreted as a rescaled version of the logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/log_&_tanh.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Advantage of the hyperbolic tangent over the logistic function**\n",
    "\n",
    "* It has a **broader output spectrum** and ranges in the **open interval** (-1, 1)\n",
    "* This can improve the convergence of the back propagation algorithm [Neural Networks for Pattern Recognition, C. M. Bishop, Oxford University Press, pages: 500-501, 1995](https://www.microsoft.com/en-us/research/wp-content/uploads/1996/01/neural_networks_pattern_recognition.pdf)\n",
    "* In contrast, the logistic function returns an output signal that ranges in the open interval (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rectified linear unit activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ``tanh`` and ``logistic`` activations suffer from **vanishing gradient problem**\n",
    "* This means the **derivative of activations** with respect to net input **diminishes** as $z$ becomes large\n",
    "* As a result, **learning weights during the training phase** become **very slow** because the gradient terms may be **very close to zero**\n",
    "* ReLU activation addresses this issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mathematically, ReLU is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/relu.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ReLU is still a nonlinear function that is good for learning complex functions with neural networks\n",
    "* Besides this, the derivative of ReLU, with respect to its input, is always 1 for positive input values\n",
    "* Therefore, it **solves** the problem of vanishing gradients, making it **suitable for deep neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Types of ReLU\n",
    "\n",
    "For a good overview of ReLU variants (Leaky ReLU, PReLU, ELU, SELU, etc.), see:\n",
    "\n",
    "[ReLU Activation Function and Its Variants — PythonKitchen](https://www.pythonkitchen.com/relu-activation-function-and-its-variants/?utm_source=chatgpt.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss function: Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Cross entropy loss, or log loss, measures the **performance of a classification model** whose output represents a **probability**, that is, a value between 0 and 1. \n",
    "* Cross entropy loss **increases** as the predicted probability **diverges** from the actual label. So predicting a probability of for example .017 when the actual observation label is 1 would result in a **high** loss value. \n",
    "* A perfect model would have a log loss of 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Choice of cross entropy in Keras**\n",
    "\n",
    "* Binary classification problems: use binary cross entropy (``binary_crossentropy`` in Keras)\n",
    "* Multi-class classification problems: use categorical cross entropy (``categorical_crossentropy`` in Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/binary_logloss.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/cross_entropy.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/categorical_logloss.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Differences for Cross entropy:\n",
    "- **Binary Classification**:\n",
    "  - Two classes (0 or 1).\n",
    "  - One predicted probability for class 1.\n",
    "  - Loss is calculated for this single probability.\n",
    "\n",
    "- **Multiclass Classification**:\n",
    "  - More than two classes.\n",
    "  - Multiple predicted probabilities (one for each class).\n",
    "  - Loss is calculated over all classes, penalizing the model for the true class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python (dat300-env)",
   "language": "python",
   "name": "dat300-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "nbpresent": {
   "slides": {
    "e1bcc92f-87ee-4cda-abb3-a53c55aa7d3b": {
     "id": "e1bcc92f-87ee-4cda-abb3-a53c55aa7d3b",
     "layout": "manual",
     "prev": null,
     "regions": {
      "20704684-44d7-4469-9fff-2966e85ae769": {
       "attrs": {
        "height": 0.8333333333333334,
        "pad": 0.01,
        "width": 0.8333333333333333,
        "x": 0.07091973498809348,
        "y": 0.09314162519826254
       },
       "id": "20704684-44d7-4469-9fff-2966e85ae769"
      },
      "8cdfe229-1ced-4a84-968c-11f4bdfb2807": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "8cdfe229-1ced-4a84-968c-11f4bdfb2807"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
