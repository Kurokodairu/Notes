{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8b0d1366-6552-473d-9e29-2a9b1f283b8e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallelising Neural Network Training with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* backpropagation worked well with relatively shallow networks (one or two layers of hidden units (1980s)\n",
    "* but that as the networks got deeper, the networks either \n",
    "    - took an inordinate amount of time to train, or \n",
    "    - else they entirely failed to converge on a good set of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Use of backpropagation to update weights **across many** layers \n",
    "    * this leads to vanishing gradient problem\n",
    "    * root of the problem: the gradient of a given layer is the **product** of gradients at **previous** layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Chain rule leading to vanishing gradient problem** (1)\n",
    "\n",
    "* Fundamentally, the backpropagation algorithm is an implementation of the chain rule from calculus\n",
    "* The chain rule involves the multiplication of terms\n",
    "* Backpropagating an error from one neuron back to another can involve multiplying the error by a number terms with values less than 1\n",
    "* These multiplications by values less than 1 happen repeatedly as the error signal gets passed back through the network\n",
    "* This results in the error signal becoming smaller and smaller as it is backpropagated through the network\n",
    "* Indeed, the error signal often diminishes exponentially with respect to the distance from the output layer\n",
    "* The effect of this diminishing error: the weights in the early layers of a deep network are often adjusted by only a tiny (or zero) amount during each training iteration\n",
    "* In other words: the early layers \n",
    "    - either train very, very slowly \n",
    "    - or do not move away from their random starting positions at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Chain rule leading to vanishing gradient problem** (2)\n",
    "\n",
    "* However, the early layers in a neural network are vitally important to the success of the network\n",
    "* It is the neurons in these layers that learn to detect the features in the input\n",
    "* Later layers of the network use detected features as the fundamental building blocks of the representations \n",
    "* These building blocks ultimately determine the output of the network\n",
    "* The error signal that is backpropagated through the network is in fact the gradient of the error of the network\n",
    "* --> this problem of the error signal rapidly diminishing to near zero is known as the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computation of error in hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"./images/fig_07_11c.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compuation of gradients for updating weights of hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_07_11cc.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The sigmoid activation function and its derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"./images/fig_07_11d.jpg\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problem with sigmoid function as activation function\n",
    "\n",
    "* The derivative becomes very small for large positive $z$ and large negative $z$\n",
    "* Derivative largest at $0.25$ for $z = 0$ (small value in itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The hyperbolic tangent activation function and its derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"./images/fig_07_11e.jpeg\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problem with hyperbolic tanget function as activation function\n",
    "\n",
    "* The derivative becomes very small for large positive $z$ and large negative $z$\n",
    "* At least, derivative around $z = 0$ higher than for sigmoid activation function (maximum 1)\n",
    "* ==> Vanishing gradient problem still present, but maybe somewhat less severe than with sigmoid\n",
    "* Consequences\n",
    "    * same as for sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Use of backpropagation to update weights **across many** layers \n",
    "    * ==> gradients for first layers may become very small (multiplication of many values < 1 for derivative of sigmoid and hyperbolic tanget activation functions) \n",
    "* Consequences\n",
    "    * long training times\n",
    "    * ==> weights of first layers change very little to not at all (many epochs needed to update weights)\n",
    "    * ==> in practice, only weights of last layers will be updated\n",
    "    * poor performance (poor learning performance of first layer influences learning of following layers)\n",
    "    * ==> learning of last layers depend on what first layers learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting and underfitting: regularisation methods for ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**COLAB NOTEBOOK 05**</font>: [Regularisation in Keras](https://colab.research.google.com/drive/17xnyr8TsZu_wF7wQZFnQ-1C9NqxkOXb9?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "nbpresent": {
   "slides": {
    "e1bcc92f-87ee-4cda-abb3-a53c55aa7d3b": {
     "id": "e1bcc92f-87ee-4cda-abb3-a53c55aa7d3b",
     "layout": "manual",
     "prev": null,
     "regions": {
      "20704684-44d7-4469-9fff-2966e85ae769": {
       "attrs": {
        "height": 0.8333333333333334,
        "pad": 0.01,
        "width": 0.8333333333333333,
        "x": 0.07091973498809348,
        "y": 0.09314162519826254
       },
       "id": "20704684-44d7-4469-9fff-2966e85ae769"
      },
      "8cdfe229-1ced-4a84-968c-11f4bdfb2807": {
       "attrs": {
        "height": 1,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "8cdfe229-1ced-4a84-968c-11f4bdfb2807"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
